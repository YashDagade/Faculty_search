{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Users/michellesi/anaconda3/lib/python3.10/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from webdriver-manager) (22.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from requests->webdriver-manager) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michellesi/anaconda3/lib/python3.10/site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "%pip install webdriver-manager\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Literal, Optional, Tuple\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "import threading\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "SEARCH_ENGINE_ID = os.getenv(\"SEARCH_ENGINE_ID\")\n",
    "\n",
    "# Define Faculty model with additional fields for GPT-generated data\n",
    "class Faculty(BaseModel):\n",
    "    research_interests_paragraph: str\n",
    "    research_interests_as_commaseperated_list: List[str]\n",
    "    hobbies: str\n",
    "    hobbies_as_commaseperated_list: List[str]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.lydiatliu.com/', 'https://www.cs.princeton.edu/people/profile/ltliu', 'https://csweb-prod-old.cs.princeton.edu/people/profile/ltliu', 'https://www.cs.princeton.edu/news/lydia-liu-expert-social-impacts-machine-learning-has-joined-faculty', 'https://citp.princeton.edu/citp-people/lydia-liu/', 'https://scholar.google.com/citations?user=IQ2eTA8AAAAJ', 'https://gradfutures.princeton.edu/grad-stories/lydia-t-liu', 'https://www.lydiatliu.com/prospective']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "def search(query: str) -> List[str]:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Enable headless mode\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(\"https://html.duckduckgo.com/html/\")\n",
    "        \n",
    "        # Find and fill search box\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "        )\n",
    "        search_box.send_keys(query)\n",
    "        search_box.submit()\n",
    "        \n",
    "        # Wait for results and extract links\n",
    "        time.sleep(2)  # Allow time for results to load\n",
    "        results = driver.find_elements(By.CSS_SELECTOR, \"a.result__a\")\n",
    "        \n",
    "        links = []\n",
    "        for result in results[:8]:  # Get first 8 results\n",
    "            links.append(result.get_attribute('href'))\n",
    "            \n",
    "        return links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during DuckDuckGo search for query '{query}': {e}\")\n",
    "        return []\n",
    "        \n",
    "    finally:\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the function\n",
    "    results = search(\"Lydia liu princeton cs\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.freep.com/story/sports/mlb/tigers/2024/12/28/detroit-tigers-alex-bregman-mlb-free-agency/77247901007/', 'https://www.detroitnews.com/story/sports/mlb/tigers/2024/12/27/tigers-reach-agreement-with-free-agent-infielder-gleyber-torres/77257853007/', 'https://www.mlb.com/news/gleyber-torres-contract-with-tigers', 'https://www.mlb.com/tigers/news', 'https://www.sportingnews.com/us/mlb/detroit-tigers', 'https://apnews.com/article/tigers-gleyber-torres-yankees-2281de1383f52831a19d37191a49a2db', 'https://en.wikipedia.org/wiki/Tiger', 'https://www.mlb.com/news/tigers-earn-comeback-win-in-series-opener-vs-royals']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# def search(query: str) -> List[str]:\n",
    "#     search_url = \"https://html.duckduckgo.com/html/\"\n",
    "#     params = {\n",
    "#         'q': query ,\n",
    "#     }\n",
    "#     headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0'\n",
    "#     }\n",
    "#     try:\n",
    "#         response = requests.post(search_url, data=params, headers=headers, timeout=10)\n",
    "#         response.raise_for_status()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during DuckDuckGo search for query '{query}': {e}\")\n",
    "#         return []\n",
    "    \n",
    "#     print(response.text)\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     links = []\n",
    "#     for result in soup.find_all('a', {'class': 'result__a'}, href=True):\n",
    "#         links.append(result['href'])\n",
    "#         if len(links) >= 8:\n",
    "#             break\n",
    "#     print(links)\n",
    "#     return links\n",
    "\n",
    "def extract_text_with_timeout(url: str, max_chars: int = 10000, timeout: int = 10) -> Tuple[str, str]:\n",
    "    def _extract():\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if response.status_code == 403:\n",
    "                return \"\", url\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            return text[:max_chars], url\n",
    "        except Exception as e:\n",
    "            return \"\", url\n",
    "\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(_extract)\n",
    "            return future.result(timeout=timeout)\n",
    "    except TimeoutError:\n",
    "        print(f\"Timeout for {url}\")\n",
    "        return \"\", url\n",
    "\n",
    "def get_text(query: str, max_chars: int = 10000) -> Tuple[str, List[str]]:\n",
    "    urls = search(query)\n",
    "    valid_texts = []\n",
    "    valid_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        text, url = extract_text_with_timeout(url)\n",
    "        if text.strip():\n",
    "            valid_texts.append(text)\n",
    "            valid_urls.append(url)\n",
    "        # time.sleep(0.5)  # Respectful delay\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return \"No valid information found.\", []\n",
    "        \n",
    "    return \"\\n\\n\".join(valid_texts), valid_urls\n",
    "\n",
    "\n",
    "\n",
    "# trying with google for funzies - not needed\n",
    "\n",
    "def google_search(query: str, num_results: int =8) -> List[str]:\n",
    "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": SEARCH_ENGINE_ID,\n",
    "        \"q\": query,\n",
    "        \"num\": num_results,  # Max results (1-10 per request)\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(search_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        results = response.json().get(\"items\", [])\n",
    "        links = [item[\"link\"] for item in results]\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google search for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "links = search(\"tigers\")\n",
    "print(links)\n",
    "\n",
    "def get_text_w_google(query: str, max_chars: int = 10000) -> Tuple[str, List[str]]:\n",
    "    urls = google_search(query)\n",
    "    valid_texts = []\n",
    "    valid_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        text, url = extract_text_with_timeout(url)\n",
    "        if text.strip():\n",
    "            valid_texts.append(text)\n",
    "            valid_urls.append(url)\n",
    "        # time.sleep(0.5)  # Respectful delay\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return \"No valid information found.\", []\n",
    "        \n",
    "    return \"\\n\\n\".join(valid_texts), valid_urls\n",
    "\n",
    "\n",
    "# get_text_w_google(\"MIT CSAIL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personal_website = client.chat.completions.create(\n",
    "#     model=\"gpt-4o\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are an helpful assitant designed to be factually accurate\"},\n",
    "#         {\"role\": \"user\", \"content\": \"can you give me Princeton Professor Lydia Liu's non-academic intrests\"},\n",
    "#     ]\n",
    "# ).choices[0].message.content\n",
    "\n",
    "# print(personal_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_hobbies(name: str) -> Faculty:\n",
    "    base_text, _ = get_text_w_google(name)\n",
    "    \n",
    "    # Stronger and more detailed prompt\n",
    "    prompt = (\n",
    "        \"Based on the following information about the individual, provide a comprehensive and detailed \"\n",
    "        \"description of their hobbies and research interests with a special focus on hobbies The description should be greater than 200 words, \"\n",
    "        \"highlighting the fun and qualitative aspects of the person's life outside of their professional work. \"\n",
    "        \"If there are multiple people with the same name, please only consider the individual who is a professor at the named university\"\n",
    "        \"Additionally, provide a comprehensive bullet-point list of their non-academic interests. Ensure the list\"\n",
    "        f\"captures in detail the main hobbies mentioned in the text: \\n\\n {base_text} \\n\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that provides detailed and comprehensive descriptions of individuals' hobbies and personal interests based on provided information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ], \n",
    "            response_format=Faculty\n",
    "        )\n",
    "        hobbies_data = response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hobbies for {name}: {e}\")\n",
    "    \n",
    "    return hobbies_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # Load the existing CSV\n",
    "# df = pd.read_csv('mit_csail_pis.csv')\n",
    "\n",
    "# # Initialize new columns\n",
    "# df['gpt_research_interest_paragraph'] = \"\"\n",
    "# df['gpt_research_interest_bullet'] = \"\"\n",
    "# df['gpt_hobbies'] = \"\"\n",
    "# df['gpt_hobbies_bullet'] = \"\"\n",
    "\n",
    "# # Iterate over each PI to fetch and append hobbies data\n",
    "# for index, row in tqdm(df.iterrows(), total=df.__len__(), desc=\"Processing PIs\"):\n",
    "#     name = row['Principal Investigator']\n",
    "#     faculty_data = get_hobbies(name)\n",
    "    \n",
    "#     df.at[index, 'gpt_hobbies'] = faculty_data.hobbies\n",
    "#     df.at[index, 'gpt_hobbies_bullet'] = \"; \".join(faculty_data.hobbies_as_commaseperated_list)\n",
    "#     df.at[index, 'gpt_research_interest_paragraph'] = faculty_data.research_interests\n",
    "#     df.at[index, 'gpt_research_interest_bullet'] = \"; \".join(faculty_data.research_interests_as_commaseperated_list)\n",
    "\n",
    "# # Save the updated CSV\n",
    "# df.to_csv('mit_csail_pis_enriched.csv', index=False)\n",
    "# print(\"Enriched CSV file 'mit_csail_pis_enriched.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run above sell for all PIs - tried to make this fater :D\n",
    "sample_run = get_hobbies(\"ariel procaccia harvard professor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside of his professional commitments, Ariel Procaccia enjoys a rich tapestry of hobbies that enrich his life beyond academia. He has a deep appreciation for philosophy, often engaging in discussions surrounding ethical implications of technology and governance. Ariel is also an avid reader, with a particular fondness for science fiction and philosophical texts, which stimulate his imagination and intellectual curiosity. He finds joy in spending time with his family, often involving outdoor activities, which provide a balance to his rigorous academic schedule. To further connect with broader societal issues, Ariel participates in community service projects, particularly those aimed at supporting refugees and homeless initiatives, reflecting his belief in the power of technology to transform lives. These hobbies not only enhance his personal well-being but also inform his professional work, as they encourage him to remain grounded in the practical and ethical considerations of technology in society. \n",
      "\n",
      "\n",
      " ['Philosophy', 'Reading (Science Fiction and Philosophical Texts)', 'Outdoor Activities with Family', 'Community Service Projects (supporting refugees and homeless initiatives)']\n",
      "\n",
      "\n",
      "\n",
      " research interests \n",
      "\n",
      "\n",
      "\n",
      "Ariel Procaccia is a prominent computer scientist whose research encapsulates a variety of interdisciplinary fields such as artificial intelligence, economics, and computation theory. With a special focus on algorithms impacting societal decision-making, his work addresses societal challenges through the lens of computational social choice and fair division. Ariel is particularly interested in projects that not only delve into theoretical concepts but also have direct real-world applications. His innovative approach has led to the creation of websites like Spliddit, which helps individuals tackle everyday fair division problems, and Panelot, which facilitates the random selection of citizensâ€™ panels for more democratic governance. These endeavors reflect his commitment to integrating technology with ethical decision-making and social justice. \n",
      "\n",
      "\n",
      " ['Applied Mathematics', 'Economics and Computation', 'Theory of Computation', 'Artificial Intelligence', 'Computation and Society']\n"
     ]
    }
   ],
   "source": [
    "print(sample_run.hobbies, '\\n\\n\\n', sample_run.hobbies_as_commaseperated_list)\n",
    "\n",
    "print(\"\\n\\n\\n research interests \\n\\n\\n\")\n",
    "print(sample_run.research_interests_paragraph, '\\n\\n\\n' ,sample_run.research_interests_as_commaseperated_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
