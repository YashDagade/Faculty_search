{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List, Literal, Optional, Tuple\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "import threading\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "SEARCH_ENGINE_ID = os.getenv(\"SEARCH_ENGINE_ID\")\n",
    "\n",
    "# Define Faculty model with additional fields for GPT-generated data\n",
    "class Faculty(BaseModel):\n",
    "    research_interests: str\n",
    "    research_interests_as_commaseperated_list: List[str]\n",
    "    hobbies: str\n",
    "    hobbies_as_commaseperated_list: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def search(query: str) -> List[str]:\n",
    "    search_url = \"https://html.duckduckgo.com/html/\"\n",
    "    params = {\n",
    "        'q': query + \" MIT Professor hobbies\",\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(search_url, data=params, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during DuckDuckGo search for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for result in soup.find_all('a', {'class': 'result__a'}, href=True):\n",
    "        links.append(result['href'])\n",
    "        if len(links) >= 8:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "def extract_text_with_timeout(url: str, max_chars: int = 10000, timeout: int = 10) -> Tuple[str, str]:\n",
    "    def _extract():\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if response.status_code == 403:\n",
    "                return \"\", url\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            return text[:max_chars], url\n",
    "        except Exception as e:\n",
    "            return \"\", url\n",
    "\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(_extract)\n",
    "            return future.result(timeout=timeout)\n",
    "    except TimeoutError:\n",
    "        print(f\"Timeout for {url}\")\n",
    "        return \"\", url\n",
    "\n",
    "def get_text(query: str, max_chars: int = 10000) -> Tuple[str, List[str]]:\n",
    "    urls = search(query)\n",
    "    valid_texts = []\n",
    "    valid_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        text, url = extract_text_with_timeout(url)\n",
    "        if text.strip():\n",
    "            valid_texts.append(text)\n",
    "            valid_urls.append(url)\n",
    "        # time.sleep(0.5)  # Respectful delay\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return \"No valid information found.\", []\n",
    "        \n",
    "    return \"\\n\\n\".join(valid_texts), valid_urls\n",
    "\n",
    "\n",
    "\n",
    "# trying with google for funzies - not needed\n",
    "\n",
    "def google_search(query: str, num_results: int =8) -> List[str]:\n",
    "    search_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": SEARCH_ENGINE_ID,\n",
    "        \"q\": query + \" MIT Professor hobbies\",\n",
    "        \"num\": num_results,  # Max results (1-10 per request)\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(search_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        results = response.json().get(\"items\", [])\n",
    "        links = [item[\"link\"] for item in results]\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google search for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "# # Example usage:\n",
    "# links = google_search(\"MIT CSAIL\")\n",
    "# print(links)\n",
    "\n",
    "def get_text_w_google(query: str, max_chars: int = 10000) -> Tuple[str, List[str]]:\n",
    "    urls = google_search(query)\n",
    "    valid_texts = []\n",
    "    valid_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        text, url = extract_text_with_timeout(url)\n",
    "        if text.strip():\n",
    "            valid_texts.append(text)\n",
    "            valid_urls.append(url)\n",
    "        # time.sleep(0.5)  # Respectful delay\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return \"No valid information found.\", []\n",
    "        \n",
    "    return \"\\n\\n\".join(valid_texts), valid_urls\n",
    "\n",
    "\n",
    "# get_text_w_google(\"MIT CSAIL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_hobbies(name: str) -> Faculty:\n",
    "    base_text, _ = get_text_w_google(name)\n",
    "    \n",
    "    # Stronger and more detailed prompt\n",
    "    prompt = (\n",
    "        \"Based on the following information about the individual, provide a comprehensive and detailed \"\n",
    "        \"description of their hobbies and resrach interests with a special focus on hobbies The description should be greater than 200 words, \"\n",
    "        \"highlighting the fun and qualitative aspects of the person's life outside of their professional work. \"\n",
    "        \"Additionally, provide a comprehensive bullet-point list of their hobbies. Ensure the list is extensive and \"\n",
    "        f\"captures all possible hobbies mentioned in the text: \\n\\n {base_text} \\n\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that provides detailed and comprehensive descriptions of individuals' hobbies and personal interests based on provided information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ], \n",
    "            response_format=Faculty\n",
    "        )\n",
    "        hobbies_data = response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing hobbies for {name}: {e}\")\n",
    "    \n",
    "    return hobbies_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # Load the existing CSV\n",
    "# df = pd.read_csv('mit_csail_pis.csv')\n",
    "\n",
    "# # Initialize new columns\n",
    "# df['gpt_research_interest_paragraph'] = \"\"\n",
    "# df['gpt_research_interest_bullet'] = \"\"\n",
    "# df['gpt_hobbies'] = \"\"\n",
    "# df['gpt_hobbies_bullet'] = \"\"\n",
    "\n",
    "# # Iterate over each PI to fetch and append hobbies data\n",
    "# for index, row in tqdm(df.iterrows(), total=df.__len__(), desc=\"Processing PIs\"):\n",
    "#     name = row['Principal Investigator']\n",
    "#     faculty_data = get_hobbies(name)\n",
    "    \n",
    "#     df.at[index, 'gpt_hobbies'] = faculty_data.hobbies\n",
    "#     df.at[index, 'gpt_hobbies_bullet'] = \"; \".join(faculty_data.hobbies_as_commaseperated_list)\n",
    "#     df.at[index, 'gpt_research_interest_paragraph'] = faculty_data.research_interests\n",
    "#     df.at[index, 'gpt_research_interest_bullet'] = \"; \".join(faculty_data.research_interests_as_commaseperated_list)\n",
    "\n",
    "# # Save the updated CSV\n",
    "# df.to_csv('mit_csail_pis_enriched.csv', index=False)\n",
    "# print(\"Enriched CSV file 'mit_csail_pis_enriched.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run above sell for all PIs - tried to make this fater :D\n",
    "sample_run = get_hobbies(\"Alan Edelman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside of his rigorous academic pursuits, Alan Edelman has cultivated a rich personal life filled with engaging hobbies that reflect his diverse interests and personality. As a devoted dog owner, he finds joy and relaxation in spending time with his little corgi, whose cheerful presence often features in the class videos for his renowned computational thinking class. This delightful companionship not only adds a playful dimension to his life but also provides a wonderful outlet for outdoor activities, such as walks and playful interactions, allowing him to escape the cerebral demands of academia. Alan also seems to embrace a philosophy of enjoying life to the fullest. He is known to appreciate the lighter side of academic life, often expressing the importance of having fun while working and teaching. This vibrant approach to life suggests that he enjoys engaging in thoughtful discussions or light-hearted banter with colleagues and students, further enriching his social interactions and fostering a collaborative atmosphere within his work environment. The intertwining of his professional and personal pursuits reflects a holistic approach to life, wherein his love for mathematics and computing integrates seamlessly with his desire for joy, connection, and curiosity beyond the classroom. \n",
      "\n",
      "\n",
      " ['Walking my corgi', 'Engaging in playful interactions with pets', 'Having fun while working', 'Connecting with colleagues and students through discussion', 'Enjoying light-hearted banter']\n",
      "\n",
      "\n",
      "\n",
      " resrach intrests \n",
      "\n",
      "\n",
      "\n",
      "High performance computing, numerical computation, linear algebra, stochastic eigenanalysis (random matrix theory). \n",
      "\n",
      "\n",
      " ['High performance computing', 'Numerical computation', 'Linear algebra', 'Stochastic eigenanalysis (random matrix theory)']\n"
     ]
    }
   ],
   "source": [
    "print(sample_run.hobbies, '\\n\\n\\n', sample_run.hobbies_as_commaseperated_list)\n",
    "\n",
    "print(\"\\n\\n\\n resrach intrests \\n\\n\\n\")\n",
    "print(sample_run.research_interests, '\\n\\n\\n', sample_run.research_interests_as_commaseperated_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
